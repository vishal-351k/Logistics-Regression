{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical**"
      ],
      "metadata": {
        "id": "QlyVSd6Aam6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q1. What is Logistic Regression, and how does it differ from Linear Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "###**Linear Regression**\n",
        "Linear Regression is a regression algorithm that predicts a continuous output variable based on one or more input features. The goal is to learn a linear relationship between the inputs and the output, which can be represented by a straight line (or a hyperplane in higher dimensions).\n",
        "\n",
        "###**Linear Regression Equation**\n",
        "y = β0 + β1x + ε\n",
        "\n",
        "Where:\n",
        "\n",
        "- y is the output variable\n",
        "- x is the input feature\n",
        "- β0 is the intercept or bias term\n",
        "- β1 is the slope coefficient\n",
        "- ε is the error term\n",
        "\n",
        "##**Logistic Regression**\n",
        "Logistic Regression, also known as Logit Regression, is a classification algorithm that predicts a binary output variable (0/1, yes/no, etc.) based on one or more input features. The goal is to learn a logistic relationship between the inputs and the output, which can be represented by a sigmoid curve (or a logistic curve).\n",
        "\n",
        "Logistic Regression Equation\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "Where:\n",
        "\n",
        "- p is the probability of the positive class (1)\n",
        "- z is the weighted sum of the input features (β0 + β1x)\n",
        "- e is the base of the natural logarithm\n",
        "\n",
        "###**Difference between Logistic Regression and Linear Regression**\n",
        "1. **Output variable:** Linear Regression predicts a continuous output variable, while Logistic Regression predicts a binary output variable.\n",
        "2. **Relationship:** Linear Regression assumes a linear relationship between the inputs and the output, while Logistic Regression assumes a logistic relationship.\n",
        "3. **Sigmoid function:** Logistic Regression uses the sigmoid function to map the weighted sum of the input features to a probability between 0 and 1.\n",
        "4. **Cost function:** Linear Regression uses the Mean Squared Error (MSE) as the cost function, while Logistic Regression uses the Log Loss (or Cross-Entropy Loss) as the cost function.\n",
        "5. **Interpretation:** Linear Regression coefficients represent the change in the output variable for a one-unit change in the input feature, while Logistic Regression coefficients represent the change in the log-odds of the positive class for a one-unit change in the input feature.\n"
      ],
      "metadata": {
        "id": "H7LqgCmcamgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q2 What is the mathematical equation of Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The mathematical equation for Logistic Regression is:\n",
        "\n",
        "Logistic Regression Equation\n",
        "p = 1 / (1 + e^(-z))\n",
        "\n",
        "Where:\n",
        "\n",
        "- p is the probability of the positive class (1)\n",
        "- e is the base of the natural logarithm (approximately 2.718)\n",
        "- z is the weighted sum of the input features, calculated as:\n",
        "\n",
        "z = β0 + β1x1 + β2x2 + … + βnxn\n",
        "\n",
        "Where:\n",
        "\n",
        "- β0 is the intercept or bias term\n",
        "- β1, β2, …, βn are the coefficients for each input feature\n",
        "- x1, x2, …, xn are the input features\n",
        "\n",
        "Sigmoid Function\n",
        "The Logistic Regression equation uses the sigmoid function to map the weighted sum (z) to a probability (p) between 0 and 1:\n",
        "\n",
        "σ(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "The sigmoid function has an S-shaped curve, which allows the model to predict probabilities that are close to 0 or 1.\n",
        "\n",
        "Log-Odds\n",
        "The weighted sum (z) can also be interpreted as the log-odds of the positive class:\n",
        "\n",
        "z = log(p / (1-p))\n",
        "\n",
        "Where p is the probability of the positive class. The log-odds is a convenient way to represent the probability, as it can take on any real value.\n",
        "\n",
        "Logistic Regression Model\n",
        "The Logistic Regression model can be represented as:\n",
        "\n",
        "p = σ(β0 + β1x1 + β2x2 + … + βnxn)\n",
        "\n",
        "Where σ is the sigmoid function. The model predicts the probability of the positive class (p) based on the input features (x1, x2, …, xn) and the model coefficients (β0, β1, β2, …, βn)."
      ],
      "metadata": {
        "id": "uY1BVoUKc73d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q3. Why do we use the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "Amswer:\n",
        "The Sigmoid function is used in Logistic Regression for several reasons:\n",
        "\n",
        "1. **Output Probability:**\n",
        "The Sigmoid function maps any real-valued number to a value between 0 and 1, which is ideal for representing probabilities. In Logistic Regression, we want to predict the probability of an event occurring (e.g., 1) or not occurring (e.g., 0).\n",
        "\n",
        "2. **Non-Linearity:**\n",
        "The Sigmoid function is non-linear, which allows the model to learn complex relationships between the input features and the output probability. Non-linearity is essential in Logistic Regression, as it enables the model to separate classes that are not linearly separable.\n",
        "\n",
        "3. **Differentiability:**\n",
        "The Sigmoid function is differentiable, which is necessary for optimizing the model using gradient-based methods. The derivative of the Sigmoid function is used to compute the gradients of the loss function with respect to the model parameters.\n",
        "\n",
        "4. **Interpretability:**\n",
        "The Sigmoid function has an intuitive interpretation. The output of the Sigmoid function can be interpreted as the probability of the positive class. This interpretability is useful for understanding the predictions made by the model.\n",
        "\n",
        "5. **Mathematical Convenience:**\n",
        "The Sigmoid function has a convenient mathematical form, which makes it easy to work with. The Sigmoid function can be written as:\n",
        "\n",
        "σ(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "This form allows for efficient computation and optimization of the model.\n",
        "\n",
        "Alternatives to Sigmoid\n",
        "While the Sigmoid function is widely used in Logistic Regression, there are alternative functions that can be used, such as:\n",
        "\n",
        "- Tanh (Hyperbolic Tangent)\n",
        "- ReLU (Rectified Linear Unit)\n",
        "- Softmax (for multi-class classification)\n"
      ],
      "metadata": {
        "id": "RlS3heT0xzET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q4. What is the cost function of Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The cost function of Logistic Regression is called the Log Loss or Cross-Entropy Loss. It measures the difference between the predicted probabilities and the true labels.\n",
        "\n",
        "Log Loss Equation\n",
        "The Log Loss equation is:\n",
        "\n",
        "L(y, p) = -[y * log(p) + (1-y) * log(1-p)]\n",
        "\n",
        "Where:\n",
        "\n",
        "- L is the Log Loss\n",
        "- y is the true label (0 or 1)\n",
        "- p is the predicted probability\n",
        "\n",
        "The Log Loss function has two parts:\n",
        "\n",
        "1. y * log(p): This term measures the loss when the true label is 1. The goal is to maximize the log probability of the positive class.\n",
        "2. (1-y) * log(1-p): This term measures the loss when the true label is 0. The goal is to maximize the log probability of the negative class.\n",
        "\n",
        "**Properties:**\n",
        "The Log Loss function has several desirable properties:\n",
        "\n",
        "- **Convexity:** The Log Loss function is convex, which means that it has a single global minimum.\n",
        "- **Differentiability:** The Log Loss function is differentiable, which makes it easy to optimize using gradient-based methods.\n",
        "- **Interpretability:** The Log Loss function has a clear interpretation as a measure of the difference between predicted probabilities and true labels.\n",
        "\n",
        "**Optimization:**\n",
        "The goal of Logistic Regression is to minimize the Log Loss function. This is typically done using gradient-based optimization algorithms, such as Gradient Descent or Stochastic Gradient Descent."
      ],
      "metadata": {
        "id": "YSJ88XeLzXe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q5. What is Regularization in Logistic Regression? Why is it needed?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the Log Loss function.\n",
        "\n",
        "####**Need for Regularization:**\n",
        "Regularization is needed to address the following issues:\n",
        "\n",
        "1. **Overfitting:** When the model is too complex and fits the training data too closely, it may not generalize well to new, unseen data. Regularization helps to reduce overfitting by adding a penalty term to the Log Loss function.\n",
        "2. **Multicollinearity:** When the features are highly correlated, the model may become unstable and prone to overfitting. Regularization helps to reduce the impact of multicollinearity.\n",
        "3. **Feature selection:** Regularization can help to select the most relevant features by shrinking the coefficients of the less important features.\n",
        "\n",
        "**Types of Regularization**\n",
        "There are two main types of regularization:\n",
        "\n",
        "1. **L1 Regularization (Lasso):** This adds a penalty term to the Log Loss function that is proportional to the absolute value of the coefficients.\n",
        "2. **L2 Regularization (Ridge):** This adds a penalty term to the Log Loss function that is proportional to the square of the coefficients.\n",
        "\n",
        "Regularized Log Loss Function\n",
        "The regularized Log Loss function can be written as:\n",
        "\n",
        "L(y, p) = -[y * log(p) + (1-y) * log(1-p)] + α * ||w||\n",
        "\n",
        "Where:\n",
        "\n",
        "- L is the regularized Log Loss\n",
        "- y is the true label\n",
        "- p is the predicted probability\n",
        "- α is the regularization parameter\n",
        "- ||w|| is the norm of the weight vector (L1 or L2)\n",
        "\n",
        "**Hyperparameter Tuning**\n",
        "The regularization parameter (α) is a hyperparameter that needs to be tuned. A common approach is to use cross-validation to evaluate the model's performance for different values of α.\n",
        "\n",
        "**Benefits of Regularization\n",
        "Regularization can help to:**\n",
        "\n",
        "- Reduce overfitting\n",
        "- Improve model interpretability\n",
        "- Select the most relevant features\n",
        "- Improve model generalization"
      ],
      "metadata": {
        "id": "-EfoDdgRzv7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Lasso, Ridge, and Elastic Net are three popular regularization techniques used in linear regression to prevent overfitting and improve model generalizability.\n",
        "\n",
        "**Lasso Regression (L1 Regularization)**\n",
        "Lasso regression uses L1 regularization, which adds a penalty term to the loss function that is proportional to the absolute value of the coefficients.\n",
        "\n",
        "**Lasso Regression Equation:**\n",
        "Loss = (y - Xw)^2 + α * ||w||1\n",
        "\n",
        "Where:\n",
        "\n",
        "- Loss is the Lasso regression loss function\n",
        "- y is the target variable\n",
        "- X is the feature matrix\n",
        "- w is the weight vector\n",
        "- α is the regularization parameter\n",
        "- ||w||1 is the L1 norm of the weight vector (sum of absolute values)\n",
        "\n",
        "**Properties of Lasso Regression**\n",
        "- Sparsity: Lasso regression can set some coefficients to zero, resulting in a sparse model.\n",
        "- Feature selection: Lasso regression can be used for feature selection by setting the coefficients of irrelevant features to zero.\n",
        "\n",
        "**Ridge Regression (L2 Regularization)**\n",
        "Ridge regression uses L2 regularization, which adds a penalty term to the loss function that is proportional to the square of the coefficients.\n",
        "\n",
        "**Ridge Regression Equation:**\n",
        "Loss = (y - Xw)^2 + α * ||w||2^2\n",
        "\n",
        "Where:\n",
        "\n",
        "- Loss is the Ridge regression loss function\n",
        "- y is the target variable\n",
        "- X is the feature matrix\n",
        "- w is the weight vector\n",
        "- α is the regularization parameter\n",
        "- ||w||2 is the L2 norm of the weight vector (Euclidean norm)\n",
        "\n",
        "**Properties of Ridge Regression**\n",
        "- Shrinkage: Ridge regression shrinks the coefficients of the features, but does not set any coefficients to zero.\n",
        "- Stability: Ridge regression can improve the stability of the model by reducing the impact of correlated features.\n",
        "\n",
        "**Elastic Net Regression**\n",
        "Elastic Net regression combines the benefits of Lasso and Ridge regression by using a combination of L1 and L2 regularization.\n",
        "\n",
        "**Elastic Net Regression Equation:**\n",
        "Loss = (y - Xw)^2 + α * (λ * ||w||1 + (1-λ) * ||w||2^2)\n",
        "\n",
        "Where:\n",
        "\n",
        "- Loss is the Elastic Net regression loss function\n",
        "- y is the target variable\n",
        "- X is the feature matrix\n",
        "- w is the weight vector\n",
        "- α is the regularization parameter\n",
        "- λ is the Elastic Net mixing parameter (between 0 and 1)\n",
        "- ||w||1 is the L1 norm of the weight vector\n",
        "- ||w||2 is the L2 norm of the weight vector\n",
        "\n",
        "**Properties  ofElastic Net Regression**\n",
        "- Sparsity: Elastic Net regression can set some coefficients to zero, like Lasso regression.\n",
        "- Shrinkage: Elastic Net regression can shrink the coefficients of the features, like Ridge regression.\n",
        "- Flexibility: Elastic Net regression allows for a flexible combination of L1 and L2 regularization.\n"
      ],
      "metadata": {
        "id": "stmRbUVp3Gqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7. When should we use Elastic Net instead of Lasso or Ridge?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Elastic Net is a versatile regularization technique that combines the benefits of Lasso and Ridge regression. Here are some scenarios where you might prefer Elastic Net over Lasso or Ridge:\n",
        "\n",
        "1. **Correlated Features**\n",
        "When features are highly correlated, Lasso may arbitrarily select one feature and set the others to zero. Elastic Net can handle correlated features more effectively by shrinking the coefficients of all correlated features.\n",
        "\n",
        "2. **Group Sparsity**\n",
        "In some cases, you may want to select groups of features instead of individual features. Elastic Net can achieve group sparsity by setting the coefficients of entire groups to zero.\n",
        "\n",
        "3. **Flexibility**\n",
        "Elastic Net offers a flexible approach to regularization by allowing you to adjust the mixing parameter (λ) to control the balance between L1 and L2 regularization.\n",
        "\n",
        "4. **Handling High-Dimensional Data**\n",
        "Elastic Net can handle high-dimensional data more effectively than Lasso or Ridge alone. By combining L1 and L2 regularization, Elastic Net can reduce the impact of irrelevant features and improve model interpretability.\n",
        "\n",
        "5. **Robustness to Outliers**\n",
        "Elastic Net can be more robust to outliers than Lasso or Ridge alone. By shrinking the coefficients of features with large absolute values, Elastic Net can reduce the impact of outliers on the model.\n",
        "\n",
        "When to prefer Lasso or Ridge over Elastic Net**\n",
        "While Elastic Net is a powerful regularization technique, there are scenarios where you might prefer Lasso or Ridge:\n",
        "\n",
        "- **Simple Models:** If you have a simple model with few features, Lasso or Ridge might be sufficient.\n",
        "- **Interpretability:** If interpretability is a top priority, Lasso might be preferred due to its ability to set coefficients to zero.\n",
        "- **Computational Efficiency:** If computational efficiency is a concern, Ridge might be preferred due to its simplicity and speed.\n",
        "\n"
      ],
      "metadata": {
        "id": "gaWERcN09nxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8. What is the impact of the regularization parameter (λ) in Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "The regularization parameter (λ) in Logistic Regression controls the strength of the regularization term, which adds a penalty to the Log Loss function for large weight values.\n",
        "\n",
        "**Impact of λ on the model:**\n",
        "1. **λ = 0:** No regularization is applied, and the model becomes a standard Logistic Regression model.\n",
        "2. **Small λ (e.g., 0.01):** A small amount of regularization is applied, which can help to prevent overfitting, but may not have a significant impact on the model.\n",
        "3. **Medium λ (e.g., 1):** A moderate amount of regularization is applied, which can help to reduce overfitting and improve the model's generalization performance.\n",
        "4. **Large λ (e.g., 100):** A strong regularization is applied, which can lead to underfitting, as the model may become too simple.\n",
        "\n",
        "**Effects of λ on the model's performance:**\n",
        "1. **Training accuracy:** Increasing λ can lead to a decrease in training accuracy, as the model becomes more regularized.\n",
        "2. **Test accuracy:** Increasing λ can lead to an increase in test accuracy, as the model becomes more generalized and less prone to overfitting.\n",
        "3. **Model complexity:** Increasing λ can lead to a decrease in model complexity, as the model becomes more regularized and less prone to overfitting.\n",
        "\n",
        "**Choosing the optimal value of λ:**\n",
        "1. **Cross-validation:** Use cross-validation to evaluate the model's performance on a holdout set and choose the value of λ that results in the best performance.\n",
        "2. **Grid search:** Perform a grid search over a range of λ values and choose the value that results in the best performance.\n",
        "3. **Bayesian optimization:** Use Bayesian optimization to search for the optimal value of λ.\n"
      ],
      "metadata": {
        "id": "TPrgG5DJx0FL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q9. What are the key assumptions of Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Logistic Regression is a powerful statistical model, but it relies on certain assumptions to ensure accurate and reliable results. Here are the key assumptions:\n",
        "\n",
        "1. **Binary Dependent Variable**\n",
        "The dependent variable (target variable) should be binary, meaning it has only two possible outcomes (e.g., 0/1, yes/no, etc.).\n",
        "\n",
        "2. **Independence of Observations**\n",
        "Each observation should be independent of the others. This means that the data points should not be paired or matched in any way.\n",
        "\n",
        "3. **Linearity in the Log-Odds**\n",
        "The log-odds of the dependent variable should be linearly related to the independent variables. This assumption can be checked using a logistic regression diagnostic plot.\n",
        "\n",
        "4. **No Multicollinearity**\n",
        "The independent variables should not be highly correlated with each other. Multicollinearity can lead to unstable estimates of the regression coefficients.\n",
        "\n",
        "5. **Homoscedasticity**\n",
        "The variance of the dependent variable should be constant across all levels of the independent variables.\n",
        "\n",
        "6. **No Outliers**\n",
        "The data should not contain outliers, which can affect the stability of the model.\n",
        "\n",
        "7. **Correct Model Specification**\n",
        "The model should be correctly specified, meaning that all relevant independent variables are included and that the functional form of the model is correct.\n",
        "\n",
        "8. **No Overdispersion**\n",
        "The data should not exhibit overdispersion, meaning that the variance of the dependent variable is not greater than the mean.\n",
        "\n",
        "Diagnostics to Check Assumptions\n",
        "To check these assumptions, you can use various diagnostic plots and tests, such as:\n",
        "\n",
        "- Logistic regression diagnostic plots (e.g., residual plots, influence plots)\n",
        "- Variance inflation factor (VIF) to check for multicollinearity\n",
        "- Cook's distance to check for outliers\n",
        "- Hosmer-Lemeshow test to check for goodness of fit\n"
      ],
      "metadata": {
        "id": "zJuzQsdUygna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q10. What are some alternatives to Logistic Regression for classification tasks?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "While Logistic Regression is a popular and effective algorithm for classification tasks, there are several alternatives you can consider, depending on the nature of your data and the specific problem you're trying to solve. Here are some alternatives:\n",
        "\n",
        "1. **Decision Trees**\n",
        "Decision Trees are a popular alternative to Logistic Regression. They work by recursively partitioning the data into smaller subsets based on the values of the input features.\n",
        "\n",
        "2. **Random Forests**\n",
        "Random Forests are an ensemble learning method that combines multiple Decision Trees to improve the accuracy and robustness of the model.\n",
        "\n",
        "3. **Support Vector Machines (SVMs)**\n",
        "SVMs are a powerful algorithm for classification tasks. They work by finding the hyperplane that maximally separates the classes in the feature space.\n",
        "\n",
        "4. **K-Nearest Neighbors (KNN)**\n",
        "KNN is a simple yet effective algorithm that classifies new instances based on the majority vote of their k-nearest neighbors.\n",
        "\n",
        "5. **Naive Bayes**\n",
        "Naive Bayes is a probabilistic algorithm that assumes independence between the input features. It's a simple and effective algorithm for classification tasks.\n",
        "\n",
        "6. **Gradient Boosting**\n",
        "Gradient Boosting is an ensemble learning method that combines multiple weak models to create a strong predictive model.\n",
        "\n",
        "7. **Neural Networks**\n",
        "Neural Networks are a powerful algorithm for classification tasks. They consist of multiple layers of interconnected nodes (neurons) that process the input data.\n",
        "\n",
        "8. **XGBoost**\n",
        "XGBoost is an optimized distributed gradient boosting library. It's a powerful algorithm for classification tasks that's known for its speed and accuracy.\n",
        "\n",
        "**Choosing an Alternative**\n",
        "\n",
        "When choosing an alternative to Logistic Regression, consider the following factors:\n",
        "\n",
        "- Data size and complexity: Larger datasets may benefit from more complex algorithms like Random Forests or Gradient Boosting.\n",
        "- Feature types: Algorithms like SVMs and Neural Networks can handle non-linear relationships between features.\n",
        "- Class imbalance: Algorithms like Random Forests and Gradient Boosting can handle class imbalance.\n",
        "- Interpretability: Algorithms like Logistic Regression and Decision Trees offer more interpretable results.\n"
      ],
      "metadata": {
        "id": "d4pOl2I2zOhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q11.What are Classification Evaluation Metrics?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Classification evaluation metrics are used to assess the performance of a classification model. Here are some common classification evaluation metrics:\n",
        "\n",
        "1. **Accuracy**\n",
        "Accuracy is the proportion of correctly classified instances out of all instances in the dataset.\n",
        "\n",
        "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "\n",
        "Where:\n",
        "\n",
        "- TP: True Positives\n",
        "- TN: True Negatives\n",
        "- FP: False Positives\n",
        "- FN: False Negatives\n",
        "\n",
        "2. **Precision**\n",
        "Precision is the proportion of true positives out of all predicted positive instances.\n",
        "\n",
        "Precision = TP / (TP + FP)\n",
        "\n",
        "3. **Recall**\n",
        "Recall is the proportion of true positives out of all actual positive instances.\n",
        "\n",
        "Recall = TP / (TP + FN)\n",
        "\n",
        "4. **F1-Score**\n",
        "F1-Score is the harmonic mean of precision and recall.\n",
        "\n",
        "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "\n",
        "5. **ROC-AUC**\n",
        "ROC-AUC (Receiver Operating Characteristic-Area Under the Curve) measures the model's ability to distinguish between positive and negative classes.\n",
        "\n",
        "6. **Confusion Matrix**\n",
        "A confusion matrix is a table used to evaluate the performance of a classification model. It displays the number of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "7. **Specificity**\n",
        "Specificity is the proportion of true negatives out of all actual negative instances.\n",
        "\n",
        "Specificity = TN / (TN + FP)\n",
        "\n",
        "8. **Sensitivity**\n",
        "Sensitivity is the proportion of true positives out of all actual positive instances.\n",
        "\n",
        "Sensitivity = TP / (TP + FN)\n",
        "\n",
        "9. **False Positive Rate**\n",
        "False Positive Rate is the proportion of false positives out of all actual negative instances.\n",
        "\n",
        "False Positive Rate = FP / (TN + FP)\n",
        "\n",
        "10. **False Negative Rate**\n",
        "False Negative Rate is the proportion of false negatives out of all actual positive instances.\n",
        "\n",
        "False Negative Rate = FN / (TP + FN)\n",
        "\n",
        "These metrics provide a comprehensive understanding of a classification model's performance and help identify areas for improvement."
      ],
      "metadata": {
        "id": "s4vnFD6Sz3cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q12.  How does class imbalance affect Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Class imbalance occurs when the number of instances in one class significantly outweighs the number of instances in the other class. This can affect Logistic Regression in several ways:\n",
        "\n",
        "1. Biased Estimates\n",
        "Logistic Regression estimates the probability of an instance belonging to the positive class. When the classes are imbalanced, the model may become biased towards the majority class, resulting in poor predictive performance on the minority class.\n",
        "\n",
        "2. Poor Predictive Performance\n",
        "Class imbalance can lead to poor predictive performance on the minority class. The model may incorrectly classify instances from the minority class as belonging to the majority class.\n",
        "\n",
        "3. Overfitting\n",
        "Class imbalance can cause overfitting, where the model becomes too specialized to the majority class and fails to generalize well to the minority class.\n",
        "\n",
        "4. Incorrect Evaluation Metrics\n",
        "Class imbalance can lead to incorrect evaluation metrics, such as accuracy, precision, and recall. These metrics may not accurately reflect the model's performance on the minority class.\n",
        "\n",
        "5. Difficulty in Model Selection\n",
        "Class imbalance can make it challenging to select the best model. The model may appear to perform well on the majority class, but poorly on the minority class.\n",
        "\n",
        "**Strategies to Handle Class Imbalance**\n",
        "1. **Oversampling the Minority Class:** Create additional instances of the minority class to balance the classes.\n",
        "2. **Undersampling the Majority Class:** Remove instances from the majority class to balance the classes.\n",
        "3. **SMOTE (Synthetic Minority Over-sampling Technique):** Create synthetic instances of the minority class using interpolation.\n",
        "4. **Cost-Sensitive Learning:** Assign different costs to misclassification errors for each class.\n",
        "5. **Ensemble Methods:** Combine multiple models trained on different subsets of the data.\n",
        "6. **Anomaly Detection:** Treat the minority class as anomalies and use anomaly detection algorithms.\n",
        "\n",
        "**Techniques to Evaluate Model Performance**\n",
        "1. **Precision-Recall Curve:** Plot the precision and recall of the model at different thresholds.\n",
        "2. **ROC-AUC Curve:** Plot the true positive rate and false positive rate of the model at different thresholds.\n",
        "3. **F1-Score:** Calculate the harmonic mean of precision and recall.\n",
        "4. **Cohen's Kappa:** Calculate the agreement between the predicted and actual classes, adjusted for chance.\n"
      ],
      "metadata": {
        "id": "ki5Ck5Ef6xGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q13. What is Hyperparameter Tuning in Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Hyperparameter tuning in Logistic Regression is the process of selecting the optimal hyperparameters for a Logistic Regression model to achieve the best possible performance.\n",
        "\n",
        "Hyperparameters are parameters that are set before training a model, as opposed to model parameters, which are learned during training. In Logistic Regression, common hyperparameters include:\n",
        "\n",
        "1. **Regularization strength (C):** Controls the strength of regularization.\n",
        "2. **Regularization type (L1, L2, or Elastic Net):** Specifies the type of regularization.\n",
        "3. **Max iterations:** Sets the maximum number of iterations for the optimization algorithm.\n",
        "4. **Tolerance:** Specifies the convergence tolerance for the optimization algorithm.\n",
        "\n",
        "\n",
        "**Importance of Hyperparameter:**\n",
        "\n",
        "1. **Improved model performance:** Hyperparameter tuning can significantly improve the performance of a Logistic Regression model.\n",
        "2. **Avoiding overfitting:** Hyperparameter tuning can help avoid overfitting by selecting the optimal regularization strength.\n",
        "3. **Model interpretability:** Hyperparameter tuning can help select the most informative features.\n",
        "\n",
        "**Hyperparameter Tuning Techniques**\n",
        "1. **Grid Search:** Exhaustively searches through a specified range of hyperparameters.\n",
        "2. **Random Search:** Randomly samples hyperparameters from a specified distribution.\n",
        "3. **Bayesian Optimization:** Uses a probabilistic approach to search for the optimal hyperparameters.\n",
        "4. **Cross-Validation:** Evaluates model performance on unseen data to avoid overfitting.\n",
        "\n",
        "**Tools for Hyperparameter Tuning**\n",
        "1. **Scikit-learn:** Provides a GridSearchCV class for hyperparameter tuning.\n",
        "2. **Hyperopt:** A Python library for Bayesian optimization.\n",
        "3. Optuna: A Python library for Bayesian optimization.\n",
        "\n",
        "**Best Practices for Hyperparameter Tuning**\n",
        "1. Split data into training and validation sets: Use a separate validation set to evaluate model performance during hyperparameter tuning.\n",
        "2. Monitor performance metrics: Track performance metrics, such as accuracy, precision, and recall, during hyperparameter tuning.\n",
        "3. Use a suitable hyperparameter tuning technique: Choose a technique that suits the problem and dataset.\n",
        "4. Avoid over-tuning: Avoid over-tuning by limiting the number of hyperparameter combinations."
      ],
      "metadata": {
        "id": "xS1aiZde76CX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q14. What are different solvers in Logistic Regression? Which one should be used?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "In Logistic Regression, solvers are algorithms used to optimize the model's parameters to minimize the loss function. Here are some common solvers:\n",
        "\n",
        "1. Newton's Method\n",
        "Newton's Method is an optimization algorithm that uses the Hessian matrix to converge to the optimal solution. It's a popular solver for Logistic Regression.\n",
        "\n",
        "2. Quasi-Newton Methods (BFGS, LBFGS)\n",
        "Quasi-Newton Methods are approximations of Newton's Method that don't require computing the Hessian matrix. BFGS (Broyden-Fletcher-Goldfarb-Shanno) and LBFGS (Limited-memory BFGS) are popular Quasi-Newton Methods.\n",
        "\n",
        "3. Gradient Descent (GD)\n",
        "Gradient Descent is a first-order optimization algorithm that iteratively updates the model's parameters to minimize the loss function.\n",
        "\n",
        "4. Stochastic Gradient Descent (SGD)\n",
        "Stochastic Gradient Descent is a variant of Gradient Descent that uses a single example or a mini-batch to update the model's parameters.\n",
        "\n",
        "5. Liblinear\n",
        "Liblinear is a linear solver that uses a trust region Newton method. It's a popular solver for large-scale linear classification problems.\n",
        "\n",
        "**Choosing a Solver\n",
        "When choosing a solver, consider the following factors:**\n",
        "\n",
        "1. Dataset size: For large datasets, use Liblinear, SGD, or LBFGS.\n",
        "2. Computational resources: For limited computational resources, use GD or SGD.\n",
        "3. Convergence speed: For fast convergence, use Newton's Method or Quasi-Newton Methods.\n",
        "4. Regularization: For problems with regularization, use Liblinear or LBFGS.\n",
        "\n",
        "**Default Solver**\n",
        "\n",
        "In scikit-learn, the default solver for Logistic Regression is:\n",
        "\n",
        "- liblinear for small datasets (n_samples < 10000)\n",
        "- sag (Stochastic Average Gradient) for large datasets (n_samples >= 10000)\n",
        "\n"
      ],
      "metadata": {
        "id": "6EoQSyY78_Py"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q15. How is Logistic Regression extended for multiclass classification?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Logistic Regression can be extended for multiclass classification using several approaches:\n",
        "\n",
        "1. **One-vs-Rest (OvR)**\n",
        "\n",
        "In OvR, a separate Logistic Regression model is trained for each class, where the positive class is the class of interest and the negative class is the combination of all other classes.\n",
        "\n",
        "2. **One-vs-One (OvO)**\n",
        "\n",
        "In OvO, a separate Logistic Regression model is trained for each pair of classes. The class with the highest number of wins is predicted as the final class.\n",
        "\n",
        "3. **Multinomial Logistic Regression**\n",
        "\n",
        "Multinomial Logistic Regression is a generalization of Logistic Regression for multiclass classification. It uses a single model to predict the probabilities of all classes.\n",
        "\n",
        "4. **Softmax Regression**\n",
        "\n",
        "Softmax Regression is a type of Multinomial Logistic Regression that uses the softmax function to output a probability distribution over all classes.\n",
        "\n",
        "**Difference between OvR, OvO, and Multinomial Logistic Regression:**\n",
        "- **OvR:**\n",
        "\n",
        " Simple to implement, but can be biased towards the majority class.\n",
        "- **OvO:**\n",
        "\n",
        " More computationally expensive, but can handle imbalanced classes.\n",
        "- **Multinomial Logistic Regression:**\n",
        "\n",
        " More flexible and can handle multiple classes, but can be more computationally expensive.\n",
        "\n",
        "**Advantages of Multinomial Logistic Regression:**\n",
        "\n",
        "- **Efficient:**\n",
        "\n",
        " Can handle multiple classes using a single model.\n",
        "- **Flexible:**\n",
        "\n",
        " Can handle different types of data and relationships.\n",
        "- **Interpretable:**\n",
        "\n",
        " Can provide insights into the relationships between the features and classes.\n",
        "\n",
        "**Common applications of Multinomial Logistic Regression:**\n",
        "\n",
        "- **Text classification:**\n",
        "\n",
        " Classifying text into multiple categories, such as spam vs. non-spam vs. unsure.\n",
        "- **Image classification:**\n",
        "\n",
        " Classifying images into multiple categories, such as objects, scenes, or actions.\n",
        "- **Recommendation systems:**\n",
        "\n",
        " Recommending products or services based on multiple categories, such as user preferences or behavior."
      ],
      "metadata": {
        "id": "FqblFimn9fRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q16. What are the advantages and disadvantages of Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Logistic Regression is a popular and widely used algorithm in machine learning and statistics. Here are some advantages and disadvantages:\n",
        "\n",
        "**Advantages:**\n",
        "1. **Interpretable:** Logistic Regression is easy to interpret, and the coefficients can be used to understand the relationships between the features and the target variable.\n",
        "2. **Efficient:** Logistic Regression is computationally efficient and can be trained on large datasets.\n",
        "3. **Robust:** Logistic Regression is robust to noise and outliers in the data.\n",
        "4. **Wide applicability:** Logistic Regression can be used for binary classification problems, and can also be extended to multi-class classification problems.\n",
        "5. **Simple to implement:** Logistic Regression is a simple algorithm to implement, and is widely available in most machine learning libraries.\n",
        "\n",
        "**Disadvantages:**\n",
        "1. **Assumes linearity:** Logistic Regression assumes a linear relationship between the features and the log-odds of the target variable.\n",
        "2. **Assumes independence:** Logistic Regression assumes that the features are independent of each other.\n",
        "3. **Not suitable for complex relationships:** Logistic Regression is not suitable for modeling complex relationships between the features and the target variable.\n",
        "4. **Can suffer from overfitting:** Logistic Regression can suffer from overfitting, especially when the number of features is large.\n",
        "5. **Not suitable for imbalanced datasets:** Logistic Regression can be biased towards the majority class when the dataset is imbalanced.\n",
        "\n",
        "**When to use Logistic Regression:**\n",
        "1. **Binary classification problems:** Logistic Regression is a good choice for binary classification problems where the target variable is a binary label.\n",
        "2. **Simple relationships:** Logistic Regression is suitable for modeling simple relationships between the features and the target variable.\n",
        "3. **Large datasets:** Logistic Regression is computationally efficient and can be trained on large datasets.\n",
        "\n",
        "**When not to use Logistic Regression:**\n",
        "1. **Complex relationships:** Logistic Regression is not suitable for modeling complex relationships between the features and the target variable.\n",
        "2. **Multi-class classification problems with complex relationships:** Logistic Regression may not be the best choice for multi-class classification problems where the relationships between the features and the target variable are complex.\n",
        "3. **Imbalanced datasets:** Logistic Regression can be biased towards the majority class when the dataset is imbalanced.\n"
      ],
      "metadata": {
        "id": "2srNSUjy_DPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q17.  What are some use cases of Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Logistic Regression is a versatile algorithm that can be applied to a wide range of problems. Here are some use cases:\n",
        "\n",
        "1. **Credit Risk Assessment**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a customer defaulting on a loan based on their credit score, income, and other factors.\n",
        "\n",
        "2. **Medical Diagnosis**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a patient having a particular disease based on their symptoms, medical history, and other factors.\n",
        "\n",
        "3. **Customer Churn Prediction**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a customer switching to a different service provider based on their usage patterns, billing history, and other factors.\n",
        "\n",
        "4. **Spam Detection**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of an email being spam based on its content, sender, and other factors.\n",
        "\n",
        "5. **Image Classification**\n",
        "\n",
        "Logistic Regression can be used to classify images into different categories, such as objects, scenes, or actions.\n",
        "\n",
        "6. **Recommendation Systems**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a user liking a particular product or service based on their past behavior and preferences.\n",
        "\n",
        "7. **Fraud Detection**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a transaction being fraudulent based on its characteristics, such as amount, location, and time.\n",
        "\n",
        "8. **Text Classification**\n",
        "\n",
        "Logistic Regression can be used to classify text into different categories, such as sentiment (positive, negative, neutral), topic (politics, sports, entertainment), or language.\n",
        "\n",
        "9. **Marketing Response Prediction**\n",
        "\n",
        "Logistic Regression can be used to predict the likelihood of a customer responding to a marketing campaign based on their demographics, behavior, and other factors.\n",
        "\n",
        "10. **Social Media Sentiment Analysis**\n",
        "\n",
        "Logistic Regression can be used to predict the sentiment of social media posts (positive, negative, neutral) based on their content and other factors.\n"
      ],
      "metadata": {
        "id": "Ex0tNZocR8Ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q18. What is the difference between Softmax Regression and Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Softmax Regression and Logistic Regression are both supervised learning algorithms used for classification problems. However, there are key differences between them:\n",
        "\n",
        "1. **Number of Classes**\n",
        "Logistic Regression is used for binary classification problems (two classes), whereas Softmax Regression is used for multi-class classification problems (more than two classes).\n",
        "\n",
        "2. **Output**\n",
        "Logistic Regression outputs a probability value between 0 and 1 for the positive class. Softmax Regression outputs a probability distribution over all classes.\n",
        "\n",
        "3. **Activation Function**\n",
        "Logistic Regression uses the sigmoid activation function, whereas Softmax Regression uses the softmax activation function.\n",
        "\n",
        "4. **Loss Function**\n",
        "Logistic Regression typically uses the log loss (binary cross-entropy) loss function, whereas Softmax Regression uses the categorical cross-entropy loss function.\n",
        "\n",
        "5. **Model Complexity**\n",
        "Softmax Regression is a more complex model than Logistic Regression, as it requires more parameters to be estimated.\n",
        "\n",
        "6. **Interpretability**\n",
        "Logistic Regression is generally more interpretable than Softmax Regression, as the coefficients can be easily interpreted as odds ratios. Softmax Regression coefficients are more difficult to interpret.\n",
        "\n",
        "**Uses**\n",
        "- **Logistic Regression:** Use for binary classification problems where the classes are mutually exclusive and the outcome is a simple yes/no or 0/1.\n",
        "- **Softmax Regression:** Use for multi-class classification problems where the classes are mutually exclusive and the outcome is one of multiple classes.\n"
      ],
      "metadata": {
        "id": "8ejpvXM_Sk19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors. Here are some considerations:\n",
        "\n",
        "1. **Number of Classes**\n",
        "- OvR: Suitable for problems with a large number of classes, as it trains a separate model for each class.\n",
        "- Softmax: Suitable for problems with a moderate number of classes (typically < 10), as it trains a single model for all classes.\n",
        "\n",
        "2. **Class Balance**\n",
        "- OvR: Can handle imbalanced classes, as each model is trained separately.\n",
        "- Softmax: Can be sensitive to class imbalance, as the model is trained on all classes simultaneously.\n",
        "\n",
        "3. **Computational Complexity**\n",
        "- OvR: Trains multiple models, which can be computationally expensive.\n",
        "- Softmax: Trains a single model, which can be more computationally efficient.\n",
        "\n",
        "4. **Model Interpretability**\n",
        "- OvR: Each model provides separate feature importance scores, which can be useful for interpretation.\n",
        "- Softmax: Provides a single set of feature importance scores, which can be more difficult to interpret.\n",
        "\n",
        "5. **Optimization**\n",
        "- OvR: Each model is optimized separately, which can lead to better performance on individual classes.\n",
        "- Softmax: The model is optimized jointly, which can lead to better overall performance.\n",
        "\n",
        "6. **Evaluation Metrics**\n",
        "- OvR: Typically evaluated using metrics like accuracy, precision, and recall for each class.\n",
        "- Softmax: Typically evaluated using metrics like accuracy, cross-entropy loss, and top-k accuracy.\n",
        "\n",
        "When to Choose Each\n",
        "- **Choose OvR when:**\n",
        "    - There are many classes (> 10).\n",
        "    - Classes are highly imbalanced.\n",
        "    - Model interpretability is important.\n",
        "- **Choose Softmax when:**\n",
        "    - There are a moderate number of classes (< 10).\n",
        "    - Classes are relatively balanced.\n",
        "    - Computational efficiency is important.\n"
      ],
      "metadata": {
        "id": "nW2VXGLkX3B2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Q20. How do we interpret coefficients in Logistic Regression?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Interpreting coefficients in Logistic Regression can be a bit tricky, but here's a step-by-step guide:\n",
        "\n",
        "1. **Understanding the coefficients**\n",
        "\n",
        "\n",
        "In Logistic Regression, the coefficients represent the change in the log-odds of the outcome variable for a one-unit change in the predictor variable, while holding all other predictor variables constant.\n",
        "\n",
        "2. **Interpreting the sign**\n",
        "\n",
        "\n",
        "- A positive coefficient indicates that an increase in the predictor variable is associated with an increase in the log-odds of the outcome variable.\n",
        "- A negative coefficient indicates that an increase in the predictor variable is associated with a decrease in the log-odds of the outcome variable.\n",
        "\n",
        "3. **Interpreting the magnitude**\n",
        "\n",
        "\n",
        "The magnitude of the coefficient represents the strength of the relationship between the predictor variable and the outcome variable.\n",
        "\n",
        "4. **Exponentiating the coefficient**\n",
        "\n",
        "\n",
        "To interpret the coefficient in terms of the odds ratio, exponentiate the coefficient:\n",
        "\n",
        "odds ratio = e^coefficient\n",
        "\n",
        "This will give you the multiplicative change in the odds of the outcome variable for a one-unit change in the predictor variable.\n",
        "\n",
        "5. **Interpreting the odds ratio**\n",
        "\n",
        "- An odds ratio greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome variable.\n",
        "- An odds ratio less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome variable.\n",
        "\n",
        "Example\n",
        "Suppose we have a Logistic Regression model that predicts the probability of a person having a heart attack based on their age, sex, and blood pressure.\n",
        "\n",
        "| Coefficient | Predictor Variable |\n",
        "| --- | --- |\n",
        "| 0.05 | Age |\n",
        "| -0.02 | Sex (1 = male, 0 = female) |\n",
        "| 0.01 | Blood Pressure |\n",
        "\n",
        "- For every one-year increase in age, the log-odds of having a heart attack increase by 0.05.\n",
        "- For males (sex = 1), the log-odds of having a heart attack decrease by 0.02 compared to females.\n",
        "- For every one-unit increase in blood pressure, the log-odds of having a heart attack increase by 0.01.\n",
        "\n",
        "Exponentiating the coefficients, we get:\n",
        "\n",
        "- e^0.05 = 1.05 (odds ratio for age)\n",
        "- e^-0.02 = 0.98 (odds ratio for sex)\n",
        "- e^0.01 = 1.01 (odds ratio for blood pressure)\n",
        "\n",
        "**Hence:**\n",
        "\n",
        "- For every one-year increase in age, the odds of having a heart attack increase by 5%.\n",
        "- For males, the odds of having a heart attack decrease by 2% compared to females.\n",
        "- For every one-unit increase in blood pressure, the odds of having a heart attack increase by 1%.\n"
      ],
      "metadata": {
        "id": "GEhxMwBqYwVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical**"
      ],
      "metadata": {
        "id": "hAUwr16xZRZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "ti4liGFLZcTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9Sc2T9QZ_5k",
        "outputId": "9eb3bb8a-dfcb-475b-d4ef-b77792c28ef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "XOySzRsdaJMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L1 regularization (Lasso)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model coefficients:\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jv68Mci7aU2p",
        "outputId": "ad9b6998-8b3d-4120-e7ef-c9778f007ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "Model coefficients:\n",
            "[[ 0.          2.36567865 -2.67515963  0.        ]\n",
            " [ 0.69398001 -1.78891928  0.24447757 -0.88935488]\n",
            " [-2.20252148 -2.8332848   3.20395668  3.72118355]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "KfFLqfloajGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model coefficients:\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA3P8Qaqassb",
        "outputId": "91f302af-1a79-4358-d308-8bc23bd97cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "Model coefficients:\n",
            "[[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "VElsMSBTa-Kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with Elastic Net Regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=1000, l1_ratio=0.5)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the coefficients of the model\n",
        "print(\"Model coefficients:\")\n",
        "print(model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rY2sVRNvbDPt",
        "outputId": "04d7f324-281b-42ff-c434-935a18fb8f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "Model coefficients:\n",
            "[[ 0.02715775  1.5962925  -2.42635941 -0.59298576]\n",
            " [ 0.          0.          0.         -0.50464381]\n",
            " [-0.78017716 -0.95707165  2.74508576  2.09726112]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "QCYCdqfObTOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model for multiclass classification using one-vs-rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwi0DqYwbdQg",
        "outputId": "9a93a71e-e07f-4b84-99a5-eea4885437b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.97\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      0.89      0.94         9\n",
            "           2       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n",
            "Confusion matrix:\n",
            "[[10  0  0]\n",
            " [ 0  8  1]\n",
            " [ 0  0 11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q6.  Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "fdC4oykubqNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "# Perform grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Print the best accuracy\n",
        "print(\"Best accuracy:\")\n",
        "print(grid_search.best_score_)\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print the accuracy on the testing data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on testing data:\")\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGSCPOrMbxc9",
        "outputId": "e2e8084e-26b9-4de2-eb67-82766379d5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:\n",
            "{'C': 1, 'penalty': 'l2'}\n",
            "Best accuracy:\n",
            "0.9666666666666666\n",
            "Accuracy on testing data:\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "dVT3XhTccFI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create a Stratified K-Fold Cross-Validation object\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Initialize a list to store the accuracy scores\n",
        "accuracy_scores = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model on the current fold's training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the current fold's testing data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy score for the current fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Append the accuracy score to the list\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "# Calculate the average accuracy score\n",
        "average_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "\n",
        "# Print the average accuracy score\n",
        "print(f\"Average accuracy: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "yvQRWbZwcMv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366efd8e-ef08-410e-9897-5b90f96e26be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "2xLRyqWFWoHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "def load_dataset(csv_file):\n",
        "    try:\n",
        "        dataset = pd.read_csv(csv_file)\n",
        "        return dataset\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Apply Logistic Regression and evaluate its accuracy\n",
        "def logistic_regression(dataset, target_variable):\n",
        "    try:\n",
        "        # Split the dataset into features (X) and target (y)\n",
        "        X = dataset.drop(target_variable, axis=1)\n",
        "        y = dataset[target_variable]\n",
        "\n",
        "        # Split the dataset into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Create a Logistic Regression model\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        # Train the model on the training data\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the testing data\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Evaluate the model's accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "        # Print the classification report\n",
        "        print(\"Classification report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Print the confusion matrix\n",
        "        print(\"Confusion matrix:\")\n",
        "        print(confusion_matrix(y_test, y_pred))\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying Logistic Regression: {str(e)}\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    csv_file = \"your_dataset.csv\"  # Replace with your CSV file\n",
        "    target_variable = \"your_target_variable\"  # Replace with your target variable\n",
        "\n",
        "    dataset = load_dataset(csv_file)\n",
        "    if dataset is not None:\n",
        "        logistic_regression(dataset, target_variable)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aN7P6ul3XHrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "D4FcC9B6XKoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create a RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(model, param_grid, cv=5, n_iter=10, random_state=42)\n",
        "\n",
        "# Perform randomized search on the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\")\n",
        "print(random_search.best_params_)\n",
        "\n",
        "# Print the best accuracy\n",
        "print(\"Best accuracy:\")\n",
        "print(random_search.best_score_)\n",
        "\n",
        "# Make predictions on the testing data using the best model\n",
        "y_pred = random_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Print the accuracy on the testing data\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on testing data:\")\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBvsLpnNXX83",
        "outputId": "f9f01c87-2a71-4475-df55-7d29c3b60d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters:\n",
            "{'solver': 'saga', 'penalty': 'l2', 'C': 10}\n",
            "Best accuracy:\n",
            "0.9666666666666668\n",
            "Accuracy on testing data:\n",
            "1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.**\n",
        "\n",
        "Answer.\n",
        "\n"
      ],
      "metadata": {
        "id": "3f8p-qplXhIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Fit and transform the target variable\n",
        "y_train_encoded = le.fit_transform(y_train)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "\n",
        "# Create a dictionary to store the OvO models\n",
        "ovo_models = {}\n",
        "\n",
        "# Train OvO models for each pair of classes\n",
        "for i in range(len(np.unique(y_train))):\n",
        "    for j in range(i + 1, len(np.unique(y_train))):\n",
        "        # Create a binary target variable for the current pair of classes\n",
        "        y_train_binary = np.where(y_train_encoded == i, 0, np.where(y_train_encoded == j, 1, 2))\n",
        "        y_test_binary = np.where(y_test_encoded == i, 0, np.where(y_test_encoded == j, 1, 2))\n",
        "\n",
        "        # Create a Logistic Regression model for the current pair of classes\n",
        "        model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "        # Train the model on the current pair of classes\n",
        "        model.fit(X_train, y_train_binary)\n",
        "\n",
        "        # Store the trained model in the dictionary\n",
        "        ovo_models[(i, j)] = model\n",
        "\n",
        "# Make predictions on the testing data using the OvO models\n",
        "y_pred = np.zeros((len(y_test),))\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    votes = np.zeros((len(np.unique(y_train)),))\n",
        "    for (class1, class2), model in ovo_models.items():\n",
        "        prediction = model.predict(X_test[i].reshape(1, -1))\n",
        "        if prediction == 0:\n",
        "            votes[class1] += 1\n",
        "        else:\n",
        "            votes[class2] += 1\n",
        "    y_pred[i] = np.argmax(votes)\n",
        "\n",
        "# Print the accuracy of the OvO model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of OvO model: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_rM1lesXshy",
        "outputId": "4621b3d7-54b5-42e3-9724-c2dc8e04b868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of OvO model: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Qza9wfFtX276"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "0ZF0P8MxYBay",
        "outputId": "08b9e6a6-532f-4d5f-d246-9b13da737cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.96\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "VJtJAJncYODV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzDVVJHWYWbF",
        "outputId": "32920df8-1448-4c2f-ab44-3c858db60b16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance:\n",
            "Accuracy: 0.96\n",
            "Precision: 0.95\n",
            "Recall: 0.99\n",
            "F1-Score: 0.97\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[39  4]\n",
            " [ 1 70]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "tGFmCAlqYVYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Generate an imbalanced classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=3, n_repeated=2, n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Compute the class weights\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "\n",
        "# Create a Logistic Regression model with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJxlBKrbZHut",
        "outputId": "1142b5e3-1274-443a-ab4e-83860445a036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.73\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.70      0.82       173\n",
            "           1       0.33      0.96      0.50        27\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.66      0.83      0.66       200\n",
            "weighted avg       0.90      0.73      0.78       200\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[121  52]\n",
            " [  1  26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.**\n",
        "Answer:"
      ],
      "metadata": {
        "id": "ej38BJ4fZXGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the Titanic dataset\n",
        "titanic = pd.read_csv(\"titanic.csv\")\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "titanic[\"Age\"] = imputer.fit_transform(titanic[[\"Age\"]])\n",
        "\n",
        "# Convert categorical variables to numerical variables\n",
        "titanic[\"Sex\"] = titanic[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
        "titanic[\"Embarked\"] = titanic[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\n",
        "\n",
        "# Drop unnecessary columns\n",
        "titanic = titanic.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = titanic.drop(\"Survived\", axis=1)\n",
        "y = titanic[\"Survived\"]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pCO2v58JaRMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "sHMNwzSeaTNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model without scaling\n",
        "model_without_scaling = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model without scaling on the training data\n",
        "model_without_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data without scaling\n",
        "y_pred_without_scaling = model_without_scaling.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy without scaling\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "print(\"Model accuracy without scaling:\")\n",
        "print(f\"Accuracy: {accuracy_without_scaling:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_without_scaling))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_without_scaling))\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale the testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model with scaling on the scaled training data\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy with scaling\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "print(\"\\nModel accuracy with scaling:\")\n",
        "print(f\"Accuracy: {accuracy_with_scaling:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_with_scaling))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_with_scaling))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32tUsF1naeM5",
        "outputId": "5a4922af-7831-4f3b-fa05-8f4f2f4701e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy without scaling:\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Model accuracy with scaling:\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "S5uq0SZjaqtn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model's performance using ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"ROC-AUC score: {roc_auc:.2f}\")\n",
        "\n",
        "# Evaluate the model's performance using accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy score: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Plot the ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "osvj3_DtazCy",
        "outputId": "af617a7f-bece-4be0-ef72-046c815df641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC score: 1.00\n",
            "Accuracy score: 0.97\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96        43\n",
            "           1       0.97      0.99      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[41  2]\n",
            " [ 1 70]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.**\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Sj0u3R6Rc5vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model with a custom learning rate (C=0.5)\n",
        "model = LogisticRegression(max_iter=1000, C=0.5)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "g_ZYt96sdBf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a1271ba-8e41-42a3-f432-ee8328adabf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "8ZfoNDSqjvBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Get the feature coefficients from the model\n",
        "feature_coefficients = model.coef_\n",
        "\n",
        "# Create a DataFrame to store the feature coefficients\n",
        "feature_coefficients_df = pd.DataFrame(feature_coefficients, columns=iris.feature_names)\n",
        "\n",
        "# Print the feature coefficients\n",
        "print(\"\\nFeature Coefficients:\")\n",
        "print(feature_coefficients_df)\n",
        "\n",
        "# Identify the most important features based on the absolute value of the coefficients\n",
        "important_features = feature_coefficients_df.abs().sum(axis=0).sort_values(ascending=False)\n",
        "\n",
        "# Print the most important features\n",
        "print(\"\\nMost Important Features:\")\n",
        "print(important_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlcU60Zaj6mV",
        "outputId": "3b8a6162-89cd-4bce-e66d-a54f12cfcca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Feature Coefficients:\n",
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0          -0.393402          0.962586          -2.375108         -0.998746\n",
            "1           0.508404         -0.254865          -0.213014         -0.775755\n",
            "2          -0.115002         -0.707721           2.588121          1.774501\n",
            "\n",
            "Most Important Features:\n",
            "petal length (cm)    5.176243\n",
            "petal width (cm)     3.549002\n",
            "sepal width (cm)     1.925172\n",
            "sepal length (cm)    1.016807\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.**\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "qVSgxEy2kSSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Evaluate the model's performance using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"\\nCohen's Kappa Score: {kappa_score:.2f}\")\n",
        "\n",
        "# Interpret the Cohen's Kappa Score\n",
        "if kappa_score < 0:\n",
        "    print(\"No agreement\")\n",
        "elif kappa_score <= 0.20:\n",
        "    print(\"Slight agreement\")\n",
        "elif kappa_score <= 0.40:\n",
        "    print(\"Fair agreement\")\n",
        "elif kappa_score <= 0.60:\n",
        "    print(\"Moderate agreement\")\n",
        "elif kappa_score <= 0.80:\n",
        "    print(\"Substantial agreement\")\n",
        "else:\n",
        "    print(\"Almost perfect agreement\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UggMa5SkfKf",
        "outputId": "d55e103b-3342-41fb-bfbc-7db96b894aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Cohen's Kappa Score: 1.00\n",
            "Almost perfect agreement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binar classification.**\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "vW4lWrLjknTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc, accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Compute the precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Compute the area under the precision-recall curve\n",
        "auc_pr = auc(recall, precision)\n",
        "print(f\"\\nArea under the precision-recall curve: {auc_pr:.2f}\")\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.plot(recall, precision, marker='.', label=f'AUC-PR = {auc_pr:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "id": "iP1tEqTakxLl",
        "outputId": "894b1324-55f9-4964-e4e7-8e421939bd60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 0.96\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.91      0.94        43\n",
            "           1       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[39  4]\n",
            " [ 1 70]]\n",
            "\n",
            "Area under the precision-recall curve: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.**\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "6tv6hU02lNnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Create a dictionary to store the accuracy of each solver\n",
        "accuracy_dict = {}\n",
        "\n",
        "# Train Logistic Regression with each solver and compare accuracy\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(max_iter=1000, solver=solver)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_dict[solver] = accuracy\n",
        "    print(f\"\\nSolver: {solver}\")\n",
        "    print(f\"Accuracy: {accuracy:.2f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Compare the accuracy of each solver\n",
        "print(\"\\nAccuracy Comparison:\")\n",
        "for solver, accuracy in accuracy_dict.items():\n",
        "    print(f\"{solver}: {accuracy:.2f}\")\n",
        "\n",
        "# Identify the solver with the highest accuracy\n",
        "best_solver = max(accuracy_dict, key=accuracy_dict.get)\n",
        "print(f\"\\nBest Solver: {best_solver}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOGXjIMVleju",
        "outputId": "f8912bdd-b5e9-459a-e904-3b729ebcfd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solver: liblinear\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Solver: saga\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Solver: lbfgs\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Accuracy Comparison:\n",
            "liblinear: 1.00\n",
            "saga: 1.00\n",
            "lbfgs: 1.00\n",
            "\n",
            "Best Solver: liblinear\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).**\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "cZBcVmPOlo_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Since MCC requires binary classification, we'll use only two classes\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Evaluate the model's performance using Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"\\nMatthews Correlation Coefficient (MCC): {mcc:.2f}\")\n",
        "\n",
        "# Interpret the MCC value\n",
        "if mcc == 1:\n",
        "    print(\"Perfect prediction\")\n",
        "elif mcc == 0:\n",
        "    print(\"No better than random prediction\")\n",
        "elif mcc == -1:\n",
        "    print(\"Worst possible prediction\")\n",
        "elif mcc > 0:\n",
        "    print(\"Good prediction\")\n",
        "else:\n",
        "    print(\"Bad prediction\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y5wnVdxl30g",
        "outputId": "433c72d8-aad2-4d09-a0fe-762e165cc560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        12\n",
            "           1       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12  0]\n",
            " [ 0  8]]\n",
            "\n",
            "Matthews Correlation Coefficient (MCC): 1.00\n",
            "Perfect prediction\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.**\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "pxG1Z23omS0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model for raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the raw training data\n",
        "model_raw.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the raw testing data\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy on raw data\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "print(\"Raw Data:\")\n",
        "print(f\"Accuracy: {accuracy_raw:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_raw))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_raw))\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Scale the testing data\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model for scaled data\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the scaled training data\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model's accuracy on scaled data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"\\nScaled Data:\")\n",
        "print(f\"Accuracy: {accuracy_scaled:.2f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_scaled))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_scaled))\n",
        "\n",
        "# Compare the accuracy of raw and scaled data\n",
        "print(\"\\nAccuracy Comparison:\")\n",
        "print(f\"Raw Data: {accuracy_raw:.2f}\")\n",
        "print(f\"Scaled Data: {accuracy_scaled:.2f}\")\n",
        "\n",
        "# Check if scaling improved accuracy\n",
        "if accuracy_scaled > accuracy_raw:\n",
        "    print(\"\\nScaling improved accuracy.\")\n",
        "else:\n",
        "    print(\"\\nScaling did not improve accuracy.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGK02acima_v",
        "outputId": "4da8c4f5-aadf-4564-e68a-289598146ec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Data:\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Scaled Data:\n",
            "Accuracy: 1.00\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Accuracy Comparison:\n",
            "Raw Data: 1.00\n",
            "Scaled Data: 1.00\n",
            "\n",
            "Scaling did not improve accuracy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.**\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "QIqrrf-xs5X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter space for GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10, 100]}\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Perform GridSearchCV to find the optimal C\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the optimal C and the corresponding model\n",
        "optimal_C = grid_search.best_params_['C']\n",
        "optimal_model = grid_search.best_estimator_\n",
        "\n",
        "# Train the optimal model on the entire training set\n",
        "optimal_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = optimal_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(f\"\\nOptimal C: {optimal_C}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6YVOlnStCuZ",
        "outputId": "6dfec5a1-c839-444b-c847-190bf66c859e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Optimal C: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.**\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Fh_UgwkttS-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Save the trained model using joblib\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "# Load the saved model using joblib\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "\n",
        "# Make predictions using the loaded model\n",
        "y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate the loaded model's accuracy\n",
        "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
        "print(f\"\\nLoaded model accuracy: {accuracy_loaded:.2f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_loaded))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_loaded))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNs9JKZBtXpg",
        "outputId": "0b063f69-a965-4d63-fee0-ed8417ced578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "\n",
            "Loaded model accuracy: 1.00\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        10\n",
            "           1       1.00      1.00      1.00         9\n",
            "           2       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        30\n",
            "   macro avg       1.00      1.00      1.00        30\n",
            "weighted avg       1.00      1.00      1.00        30\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n"
          ]
        }
      ]
    }
  ]
}